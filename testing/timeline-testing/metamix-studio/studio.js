(function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
function Do(parent) {
	var listeners = [];
	this.do = function(callback) {
		listeners.push(callback);
	};
	this.undo = function(callback) {
		listeners.splice(listeners.indexOf(callback), 1);
	};
	this.fire = function() {
		for (var v = 0; v < listeners.length; v++) {
			listeners[v].apply(parent, arguments);
		}
	};
}

if (typeof(module) === 'object') module.exports = Do;

},{}],2:[function(require,module,exports){
"use strict";

/**
 * ArrayBuffer adapter consumes binary waveform data (data format version 1).
 * It is used as a data abstraction layer by `WaveformData`.
 *
 * This is supposed to be the fastest adapter ever:
 * * **Pros**: working directly in memory, everything is done by reference
 *   (including the offsetting)
 * * **Cons**: binary data are hardly readable without data format knowledge
 *   (and this is why this adapter exists).
 *
 * Also, it is recommended to use the `fromResponseData` factory.
 *
 * @see WaveformDataArrayBufferAdapter.fromResponseData
 * @param {DataView} response_data
 * @constructor
 */

function WaveformDataArrayBufferAdapter(response_data) {
  this.data = response_data;
}

/**
 * Detects if a set of data is suitable for the ArrayBuffer adapter.
 * It is used internally by `WaveformData.create` so you should not bother using it.
 *
 * @static
 * @param {Mixed} data
 * @returns {boolean}
 */

WaveformDataArrayBufferAdapter.isCompatible = function isCompatible(data) {
  return data && typeof data === "object" && "byteLength" in data;
};

/**
 * Setup factory to create an adapter based on heterogeneous input formats.
 *
 * It is the preferred way to build an adapter instance.
 *
 * ```javascript
 * var arrayBufferAdapter = WaveformData.adapters.arraybuffer;
 * var xhr = new XMLHttpRequest();
 *
 * // .dat file generated by audiowaveform program
 * xhr.open("GET", "http://example.com/waveforms/track.dat");
 * xhr.responseType = "arraybuffer";
 * xhr.addEventListener("load", function onResponse(progressEvent){
 *  var responseData = progressEvent.target.response;
 *
 *  // doing stuff with the raw data ...
 *  // you only have access to WaveformDataArrayBufferAdapter API
 *  var adapter = arrayBufferAdapter.fromResponseData(responseData);
 *
 *  // or making things easy by using WaveformData ...
 *  // you have access WaveformData API
 *  var waveform = new WaveformData(responseData, arrayBufferAdapter);
 * });
 *
 * xhr.send();
 * ```

 * @static
 * @param {ArrayBuffer} response_data
 * @return {WaveformDataArrayBufferAdapter}
 */

WaveformDataArrayBufferAdapter.fromResponseData = function fromArrayBufferResponseData(response_data) {
  return new WaveformDataArrayBufferAdapter(new DataView(response_data));
};

/**
 * @namespace WaveformDataArrayBufferAdapter
 */

 WaveformDataArrayBufferAdapter.prototype = {
  /**
   * Returns the data format version number.
   *
   * @return {Integer} Version number of the consumed data format.
   */

  get version() {
    return this.data.getInt32(0, true);
  },

  /**
   * Indicates if the response body is encoded in 8bits.
   *
   * **Notice**: currently the adapter only deals with 8bits encoded data.
   * You should favor that too because of the smaller data network fingerprint.
   *
   * @return {boolean} True if data are declared to be 8bits encoded.
   */

  get is_8_bit() {
    return Boolean(this.data.getUint32(4, true));
  },

  /**
   * Indicates if the response body is encoded in 16bits.
   *
   * @return {boolean} True if data are declared to be 16bits encoded.
   */

  get is_16_bit() {
    return !this.is_8_bit;
  },

  /**
   * Returns the number of samples per second.
   *
   * @return {Integer} Number of samples per second.
   */

  get sample_rate() {
    return this.data.getInt32(8, true);
  },

  /**
   * Returns the scale (number of samples per pixel).
   *
   * @return {Integer} Number of samples per pixel.
   */

  get scale() {
    return this.data.getInt32(12, true);
  },

  /**
   * Returns the length of the waveform data (number of data points).
   *
   * @return {Integer} Length of the waveform data.
   */

  get length() {
    return this.data.getUint32(16, true);
  },

  /**
   * Returns a value at a specific offset.
   *
   * @param {Integer} index
   * @return {number} waveform value
   */

  at: function at_sample(index) {
    return this.data.getInt8(20 + index);
  }
};

module.exports = WaveformDataArrayBufferAdapter;

},{}],3:[function(require,module,exports){
"use strict";

module.exports = {
  arraybuffer: require("./arraybuffer"),
  object: require("./object")
};

},{"./arraybuffer":2,"./object":4}],4:[function(require,module,exports){
"use strict";

/**
 * Object adapter consumes stringified JSON or JSON waveform data (data format version 1).
 * It is used as a data abstraction layer by `WaveformData`.
 *
 * This is supposed to be a fallback for browsers not supporting ArrayBuffer:
 * * **Pros**: easy to debug response_data and quite self describing.
 * * **Cons**: slower than ArrayBuffer, more memory consumption.
 *
 * Also, it is recommended to use the `fromResponseData` factory.
 *
 * @see WaveformDataObjectAdapter.fromResponseData
 * @param {String|Object} response_data JSON or stringified JSON
 * @constructor
 */

function WaveformDataObjectAdapter(response_data) {
  this.data = response_data;
}

/**
 * Detects if a set of data is suitable for the Object adapter.
 * It is used internally by `WaveformData.create` so you should not bother using it.
 *
 * @static
 * @param {Mixed} data
 * @returns {boolean}
 */

WaveformDataObjectAdapter.isCompatible = function isCompatible(data) {
  return data && ((typeof data === "object" && "sample_rate" in data) ||
                  (typeof data === "string" && "sample_rate" in JSON.parse(data)));
};

/**
 * Setup factory to create an adapter based on heterogeneous input formats.
 *
 * It is the preferred way to build an adapter instance.
 *
 * ```javascript
 * var objectAdapter = WaveformData.adapters.object;
 * var xhr = new XMLHttpRequest();
 *
 * // .dat file generated by audiowaveform program
 * xhr.open("GET", "http://example.com/waveforms/track.json");
 * xhr.responseType = "json";
 * xhr.addEventListener("load", function onResponse(progressEvent){
 *  var responseData = progressEvent.target.response;
 *
 *  // doing stuff with the raw data ...
 *  // you only have access to WaveformDataObjectAdapter API
 *  var adapter = objectAdapter.fromResponseData(responseData);
 *
 *  // or making things easy by using WaveformData ...
 *  // you have access WaveformData API
 *  var waveform = new WaveformData(responseData, objectAdapter);
 * });
 *
 * xhr.send();
 * ```

 * @static
 * @param {String|Object} response_data JSON or stringified JSON
 * @return {WaveformDataObjectAdapter}
 */

WaveformDataObjectAdapter.fromResponseData = function fromJSONResponseData(response_data) {
  if (typeof response_data === "string") {
    return new WaveformDataObjectAdapter(JSON.parse(response_data));
  }
  else {
    return new WaveformDataObjectAdapter(response_data);
  }
};

/**
 * @namespace WaveformDataObjectAdapter
 */

WaveformDataObjectAdapter.prototype = {
  /**
   * Returns the data format version number.
   *
   * @return {Integer} Version number of the consumed data format.
   */

  get version() {
    return this.data.version || 1;
  },

  /**
   * Indicates if the response body is encoded in 8bits.
   *
   * **Notice**: currently the adapter only deals with 8bits encoded data.
   * You should favor that too because of the smaller data network fingerprint.
   *
   * @return {boolean} True if data are declared to be 8bits encoded.
   */

  get is_8_bit() {
    return this.data.bits === 8;
  },

  /**
   * Indicates if the response body is encoded in 16bits.
   *
   * @return {boolean} True if data are declared to be 16bits encoded.
   */

  get is_16_bit() {
    return !this.is_8_bit;
  },

  /**
   * Returns the number of samples per second.
   *
   * @return {Integer} Number of samples per second.
   */

  get sample_rate() {
    return this.data.sample_rate;
  },

  /**
   * Returns the scale (number of samples per pixel).
   *
   * @return {Integer} Number of samples per pixel.
   */

  get scale() {
    return this.data.samples_per_pixel;
  },

  /**
   * Returns the length of the waveform data (number of data points).
   *
   * @return {Integer} Length of the waveform data.
   */

  get length() {
    return this.data.length;
  },

  /**
   * Returns a value at a specific offset.
   *
   * @param {Integer} index
   * @return {number} waveform value
   */

  at: function at_sample(index) {
    return this.data.data[index];
  }
};

module.exports = WaveformDataObjectAdapter;

},{}],5:[function(require,module,exports){
"use strict";

var WaveformDataSegment = require("./segment");
var WaveformDataPoint = require("./point");

/**
 * Facade to iterate on audio waveform response.
 *
 * ```javascript
 * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
 *
 * var json_waveform = new WaveformData(xhr.responseText, WaveformData.adapters.object);
 *
 * var arraybuff_waveform = new WaveformData(
 *   getArrayBufferData(),
 *   WaveformData.adapters.arraybuffer
 * );
 * ```
 *
 * ## Offsets
 *
 * An **offset** is a non-destructive way to iterate on a subset of data.
 *
 * It is the easiest way to **navigate** through data without having to deal
 * with complex calculations. Simply iterate over the data to display them.
 *
 * *Notice*: the default offset is the entire set of data.
 *
 * @param {String|ArrayBuffer|Mixed} response_data Waveform data,
 *   to be consumed by the related adapter.
 * @param {WaveformData.adapter|Function} adapter Backend adapter used to manage
 *   access to the data.
 * @constructor
 */

function WaveformData(response_data, adapter) {
  /**
   * Backend adapter used to manage access to the data.
   *
   * @type {Object}
   */

  this.adapter = adapter.fromResponseData(response_data);

  /**
   * Defined segments.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   *
   * console.log(waveform.segments.speakerA); // -> undefined
   *
   * waveform.set_segment(30, 90, "speakerA");
   *
   * console.log(waveform.segments.speakerA.start); // -> 30
   * ```
   *
   * @type {Object} A hash of `WaveformDataSegment` objects.
   */

  this.segments = {};

  /**
   * Defined points.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   *
   * console.log(waveform.points.speakerA); // -> undefined
   *
   * waveform.set_point(30, "speakerA");
   *
   * console.log(waveform.points.speakerA.timeStamp); // -> 30
   * ```
   *
   * @type {Object} A hash of `WaveformDataPoint` objects.
   */

  this.points = {};

  this.offset(0, this.adapter.length);
}

/**
 * Creates an instance of WaveformData by guessing the adapter from the
 * data type. It can also accept an XMLHttpRequest response.
 *
 * ```javascript
 * var xhr = new XMLHttpRequest();
 * xhr.open("GET", "http://example.com/waveforms/track.dat");
 * xhr.responseType = "arraybuffer";
 *
 * xhr.addEventListener("load", function onResponse(progressEvent) {
 *   var waveform = WaveformData.create(progressEvent.target);
 *
 *   console.log(waveform.duration);
 * });
 *
 * xhr.send();
 * ```
 *
 * @static
 * @throws TypeError
 * @param {XMLHttpRequest|Mixed} data
 * @return {WaveformData}
 */

WaveformData.create = function createFromResponseData(data) {
  var adapter = null;
  var xhrData = null;

  if (data && typeof data === "object" && ("responseText" in data || "response" in data)) {
    xhrData = ("responseType" in data) ? data.response : (data.responseText || data.response);
  }

  Object.keys(WaveformData.adapters).some(function(adapter_id) {
    if (WaveformData.adapters[adapter_id].isCompatible(xhrData || data)) {
      adapter = WaveformData.adapters[adapter_id];
      return true;
    }
  });

  if (adapter === null) {
    throw new TypeError("Could not detect a WaveformData adapter from the input.");
  }

  return new WaveformData(xhrData || data, adapter);
};

/**
 * Public API for the Waveform Data manager.
 *
 * @namespace WaveformData
 */

WaveformData.prototype = {
  /**
   * Clamp an offset of data upon the whole response body.
   * Pros: it's just a reference, not a new array. So it's fast.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.offset_length);   // -> 150
   * console.log(waveform.min[0]);          // -> -12
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.min.length);      // -> 30
   * console.log(waveform.min[0]);          // -> -9
   * ```
   *
   * @param {Integer} start New beginning of the offset. (inclusive)
   * @param {Integer} end New ending of the offset (exclusive)
   */

  offset: function(start, end) {
    var data_length = this.adapter.length;

    if (end < 0) {
      throw new RangeError("End point must be non-negative [" + Number(end) + " < 0]");
    }

    if (end < start) {
      throw new RangeError("End point must not be before the start point [" + Number(end) + " < " + Number(start) + "]");
    }

    if (start < 0) {
      throw new RangeError("Start point must be non-negative [" + Number(start) + " < 0]");
    }

    if (start >= data_length) {
      throw new RangeError("Start point must be within range [" + Number(start) + " >= " + data_length + "]");
    }

    if (end > data_length) {
      end = data_length;
    }

    this.offset_start = start;
    this.offset_end = end;
    this.offset_length = end - start;
  },

  /**
   * Creates a new segment of data.
   * Pretty handy if you need to bookmark a duration and display it according
   * to the current offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(Object.keys(waveform.segments));          // -> []
   *
   * waveform.set_segment(10, 120);
   * waveform.set_segment(30, 90, "speakerA");
   *
   * console.log(Object.keys(waveform.segments));          // -> ['default', 'speakerA']
   * console.log(waveform.segments.default.min.length);    // -> 110
   * console.log(waveform.segments.speakerA.min.length);   // -> 60
   * ```
   *
   * @param {Integer} start Beginning of the segment (inclusive)
   * @param {Integer} end Ending of the segment (exclusive)
   * @param {String*} identifier Unique identifier. If nothing is specified,
   *   *default* will be used as a value.
   * @return {WaveformDataSegment}
   */

  set_segment: function setSegment(start, end, identifier) {
    if (identifier === undefined || identifier === null || identifier.length === 0) {
      identifier = "default";
    }

    this.segments[identifier] = new WaveformDataSegment(this, start, end);

    return this.segments[identifier];
  },

  /**
   * Creates a new point of data.
   * Pretty handy if you need to bookmark a specific point and display it
   * according to the current offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(Object.keys(waveform.points)); // -> []
   *
   * waveform.set_point(10);
   * waveform.set_point(30, "speakerA");
   *
   * console.log(Object.keys(waveform.points)); // -> ['default', 'speakerA']
   * ```
   *
   * @param {Integer} timeStamp the time to place the bookmark
   * @param {String*} identifier Unique identifier. If nothing is specified,
   *   *default* will be used as a value.
   * @return {WaveformDataPoint}
   */

  set_point: function setPoint(timeStamp, identifier) {
    if (identifier === undefined || identifier === null || identifier.length === 0) {
      identifier = "default";
    }

    this.points[identifier] = new WaveformDataPoint(this, timeStamp);

    return this.points[identifier];
  },

  /**
   * Removes a point of data.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(Object.keys(waveform.points));          // -> []
   *
   * waveform.set_point(30, "speakerA");
   * console.log(Object.keys(waveform.points));          // -> ['speakerA']
   * waveform.remove_point("speakerA");
   * console.log(Object.keys(waveform.points));          // -> []
   * ```
   *
   * @param {String*} identifier Unique identifier. If nothing is specified,
   *   *default* will be used as a value.
   * @return null
   */

  remove_point: function removePoint(identifier) {
    if (this.points[identifier]) {
      delete this.points[identifier];
    }
  },

  /**
   * Creates a new WaveformData object with resampled data.
   * Returns a rescaled waveform, to either fit the waveform to a specific
   * width, or to a specific zoom level.
   *
   * **Note**: You may specify either the *width* or the *scale*, but not both.
   * The `scale` will be deduced from the `width` you want to fit the data into.
   *
   * Adapted from Sequence::GetWaveDisplay in Audacity, with permission.
   *
   * ```javascript
   * // ...
   * var waveform = WaveformData.create({ ... });
   *
   * // fitting the data in a 500px wide canvas
   * var resampled_waveform = waveform.resample({ width: 500 });
   *
   * console.log(resampled_waveform.min.length);   // -> 500
   *
   * // zooming out on a 3 times less precise scale
   * var resampled_waveform = waveform.resample({ scale: waveform.adapter.scale * 3 });
   *
   * // partial resampling (to perform fast animations involving a resampling
   * // per animation frame)
   * var partially_resampled_waveform = waveform.resample({ width: 500, from: 0, to: 500 });
   *
   * // ...
   * ```
   *
   * @see https://code.google.com/p/audacity/source/browse/audacity-src/trunk/src/Sequence.cpp
   * @param {Number|{width: Number, scale: Number}} options Either a constraint width or a constraint sample rate
   * @return {WaveformData} New resampled object
   */

  resample: function(options) {
    if (typeof options === "number") {
      options = {
        width: options
      };
    }

    options.input_index = typeof options.input_index === "number" ? options.input_index : null;
    options.output_index = typeof options.output_index === "number" ? options.output_index : null;
    options.scale = typeof options.scale === "number" ? options.scale : null;
    options.width = typeof options.width === "number" ? options.width : null;

    var is_partial_resampling = Boolean(options.input_index) || Boolean(options.output_index);

    if (options.input_index != null && (options.input_index < 0)) {
      throw new RangeError("options.input_index should be a positive integer value. [" + options.input_index + "]");
    }

    if (options.output_index != null && (options.output_index < 0)) {
      throw new RangeError("options.output_index should be a positive integer value. [" + options.output_index + "]");
    }

    if (options.width != null && (options.width <= 0)) {
      throw new RangeError("options.width should be a strictly positive integer value. [" + options.width + "]");
    }

    if (options.scale != null && (options.scale <= 0)) {
      throw new RangeError("options.scale should be a strictly positive integer value. [" + options.scale + "]");
    }

    if (!options.scale && !options.width) {
      throw new RangeError("You should provide either a resampling scale or a width in pixel the data should fit in.");
    }

    var definedPartialOptionsCount = ["width", "scale", "output_index", "input_index"].reduce(function(count, key) {
      return count + (options[key] === null ? 0 : 1);
    }, 0);

    if (is_partial_resampling && definedPartialOptionsCount !== 4) {
      throw new Error("Some partial resampling options are missing. You provided " + definedPartialOptionsCount + " of them over 4.");
    }

    var output_data = [];
    var samples_per_pixel = options.scale || Math.floor(this.duration * this.adapter.sample_rate / options.width); // scale we want to reach
    var scale = this.adapter.scale; // scale we are coming from
    var channel_count = 2;

    var input_buffer_size = this.adapter.length; // the amount of data we want to resample i.e. final zoom want to resample all data but for intermediate zoom we want to resample subset
    var input_index = options.input_index || 0; // is this start point? or is this the index at current scale
    var output_index = options.output_index || 0; // is this end point? or is this the index at scale we want to be?
    var min = input_buffer_size ? this.min_sample(input_index) : 0; // min value for peak in waveform
    var max = input_buffer_size ? this.max_sample(input_index) : 0; // max value for peak in waveform
    var min_value = -128;
    var max_value = 127;

    if (samples_per_pixel < scale) {
      throw new Error("Zoom level " + samples_per_pixel + " too low, minimum: " + scale);
    }

    var where, prev_where, stop, value, last_input_index;

    function sample_at_pixel(x) {
      return Math.floor(x * samples_per_pixel);
    }

    function add_sample(min, max) {
      output_data.push(min, max);
    }

    while (input_index < input_buffer_size) {
      while (Math.floor(sample_at_pixel(output_index) / scale) <= input_index) {
        if (output_index) {
          add_sample(min, max);
        }

        last_input_index = input_index;

        output_index++;

        where      = sample_at_pixel(output_index);
        prev_where = sample_at_pixel(output_index - 1);

        if (where !== prev_where) {
          min = max_value;
          max = min_value;
        }
      }

      where = sample_at_pixel(output_index);
      stop = Math.floor(where / scale);

      if (stop > input_buffer_size) {
        stop = input_buffer_size;
      }

      while (input_index < stop) {
        value = this.min_sample(input_index);

        if (value < min) {
          min = value;
        }

        value = this.max_sample(input_index);

        if (value > max) {
          max = value;
        }

        input_index++;
      }

      if (is_partial_resampling && (output_data.length / channel_count) >= options.width) {
        break;
      }
    }

    if (is_partial_resampling) {
      if ((output_data.length / channel_count) > options.width &&
          input_index !== last_input_index) {
        add_sample(min, max);
      }
    }
    else if (input_index !== last_input_index) {
      add_sample(min, max);
    }

    return new WaveformData({
      version: this.adapter.version,
      samples_per_pixel: samples_per_pixel,
      length: output_data.length / channel_count,
      data: output_data,
      sample_rate: this.adapter.sample_rate
    }, WaveformData.adapters.object);
  },

  /**
   * Returns all the min peaks values.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.min.length);      // -> 150
   * console.log(waveform.min[0]);          // -> -12
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.min.length);      // -> 30
   * console.log(waveform.min[0]);          // -> -9
   * ```
   *
   * @api
   * @return {Array.<Integer>} Min values contained in the offset.
   */

  get min() {
    return this.offsetValues(this.offset_start, this.offset_length, 0);
  },

  /**
   * Returns all the max peaks values.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.max.length);      // -> 150
   * console.log(waveform.max[0]);          // -> 12
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.max.length);      // -> 30
   * console.log(waveform.max[0]);          // -> 5
   * ```
   *
   * @api
   * @return {Array.<Integer>} Max values contained in the offset.
   */

  get max() {
    return this.offsetValues(this.offset_start, this.offset_length, 1);
  },

  /**
   * Return the unpacked values for a particular offset.
   *
   * @param {Integer} start
   * @param {Integer} length
   * @param {Integer} correction The step to skip for each iteration
   *   (as the response body is [min, max, min, max...])
   * @return {Array.<Integer>}
   */

  offsetValues: function getOffsetValues(start, length, correction) {
    var adapter = this.adapter;
    var values = [];

    correction += (start * 2); // offset the positioning query

    for (var i = 0; i < length; i++) {
      values.push(adapter.at((i * 2) + correction));
    }

    return values;
  },

  /**
   * Compute the duration in seconds of the audio file.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   * console.log(waveform.duration);    // -> 10.33333333333
   *
   * waveform.offset(20, 50);
   * console.log(waveform.duration);    // -> 10.33333333333
   * ```
   *
   * @api
   * @return {number} Duration of the audio waveform, in seconds.
   */

  get duration() {
    return (this.adapter.length * this.adapter.scale) / this.adapter.sample_rate;
  },

  /**
   * Return the duration in seconds of the current offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.offset_duration);    // -> 10.33333333333
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.offset_duration);    // -> 2.666666666667
   * ```
   *
   * @api
   * @return {number} Duration of the offset, in seconds.
   */

  get offset_duration() {
    return (this.offset_length * this.adapter.scale) / this.adapter.sample_rate;
  },

  /**
   * Return the number of pixels per second.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.pixels_per_second);       // -> 93.75
   * ```
   *
   * @api
   * @return {number} Number of pixels per second.
   */

  get pixels_per_second() {
    return this.adapter.sample_rate / this.adapter.scale;
  },

  /**
   * Return the amount of time represented by a single pixel.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.seconds_per_pixel);       // -> 0.010666666666666666
   * ```
   *
   * @return {number} Amount of time (in seconds) contained in a pixel.
   */

  get seconds_per_pixel() {
    return this.adapter.scale / this.adapter.sample_rate;
  },

  /**
   * Returns a value at a specific offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.at(20));              // -> -7
   * console.log(waveform.at(21));              // -> 12
   * ```
   *
   * @proxy
   * @param {Integer} index
   * @return {number} Offset value
   */

  at: function at_sample_proxy(index) {
    return this.adapter.at(index);
  },

  /**
   * Return the pixel location for a certain time.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.at_time(0.0000000023));       // -> 10
   * ```
   * @param {number} time
   * @return {integer} Index location for a specific time.
   */

  at_time: function at_time(time) {
    return Math.floor(time * this.adapter.sample_rate / this.adapter.scale);
  },

  /**
   * Returns the time in seconds for a particular index
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.time(10));                    // -> 0.0000000023
   * ```
   *
   * @param {Integer} index
   * @return {number}
   */

  time: function time(index) {
    return index * this.adapter.scale / this.adapter.sample_rate;
  },

  /**
   * Return if a pixel lies within the current offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.in_offset(50));      // -> true
   * console.log(waveform.in_offset(120));     // -> true
   *
   * waveform.offset(100, 150);
   *
   * console.log(waveform.in_offset(50));      // -> false
   * console.log(waveform.in_offset(120));     // -> true
   * ```
   *
   * @param {number} pixel
   * @return {boolean} True if the pixel lies in the current offset, false otherwise.
   */

  in_offset: function isInOffset(pixel) {
    return pixel >= this.offset_start && pixel < this.offset_end;
  },

  /**
   * Returns a min value for a specific offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.min_sample(10));      // -> -7
   * ```
   *
   * @param {Integer} offset
   * @return {Number} Offset min value
   */

  min_sample: function getMinValue(offset) {
    return this.adapter.at(offset * 2);
  },

  /**
   * Returns a max value for a specific offset.
   *
   * ```javascript
   * var waveform = WaveformData.create({ ... });
   *
   * console.log(waveform.max_sample(10));      // -> 12
   * ```
   *
   * @param {Integer} offset
   * @return {Number} Offset max value
   */

  max_sample: function getMaxValue(offset) {
    return this.adapter.at((offset * 2) + 1);
  }
};

/**
 * Available adapters to manage the data backends.
 *
 * @type {Object}
 */

WaveformData.adapters = {};

/**
 * WaveformData Adapter Structure
 *
 * @typedef {{from: Number, to: Number, platforms: {}}}
 */

WaveformData.adapter = function WaveformDataAdapter(response_data) {
  this.data = response_data;
};

module.exports = WaveformData;

},{"./point":6,"./segment":7}],6:[function(require,module,exports){
"use strict";

/**
 * Points are an easy way to keep track bookmarks of the described audio file.
 *
 * They return values based on the actual offset. Which means if you change your offset and:
 *
 * * a point becomes **out of scope**, no data will be returned;
 * * a point is **fully included in the offset**, its whole content will be returned.
 *
 * Points are created with the `WaveformData.set_point(timeStamp, name?)` method.
 *
 * @see WaveformData.prototype.set_point
 * @param {WaveformData} context WaveformData instance
 * @param {Integer} start Initial start index
 * @param {Integer} end Initial end index
 * @constructor
 */

function WaveformDataPoint(context, timeStamp) {
  this.context = context;

  /**
   * Start index.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_point(10, "example");
   *
   * console.log(waveform.points.example.timeStamp);  // -> 10
   *
   * waveform.offset(20, 50);
   * console.log(waveform.points.example.timeStamp);  // -> 10
   *
   * waveform.offset(70, 100);
   * console.log(waveform.points.example.timeStamp);  // -> 10
   * ```
   * @type {Integer} Time Stamp of the point
   */

  this.timeStamp = timeStamp;
}

/**
 * @namespace WaveformDataPoint
 */

WaveformDataPoint.prototype = {
  /**
   * Indicates if the point has some visible part in the actual WaveformData offset.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_point(10, "example");
   *
   * console.log(waveform.points.example.visible);        // -> true
   *
   * waveform.offset(0, 50);
   * console.log(waveform.points.example.visible);        // -> true
   *
   * waveform.offset(70, 100);
   * console.log(waveform.points.example.visible);        // -> false
   * ```
   *
   * @return {Boolean} True if visible, false otherwise.
   */

  get visible() {
    return this.context.in_offset(this.timeStamp);
  }
};

module.exports = WaveformDataPoint;

},{}],7:[function(require,module,exports){
"use strict";

/**
 * Segments are an easy way to keep track of portions of the described
 * audio file.
 *
 * They return values based on the actual offset. Which means if you change your
 * offset and:
 *
 * * a segment becomes **out of scope**, no data will be returned;
 * * a segment is only **partially included in the offset**, only the visible
 *   parts will be returned;
 * * a segment is **fully included in the offset**, its whole content will be
 *   returned.
 *
 * Segments are created with the `WaveformData.set_segment(from, to, name?)`
 * method.
 *
 * @see WaveformData.prototype.set_segment
 * @param {WaveformData} context WaveformData instance
 * @param {Integer} start Initial start index
 * @param {Integer} end Initial end index
 * @constructor
 */

function WaveformDataSegment(context, start, end) {
  this.context = context;

  /**
   * Start index.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.start);  // -> 10
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.start);  // -> 10
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.start);  // -> 10
   * ```
   * @type {Integer} Initial starting point of the segment.
   */

  this.start = start;

  /**
   * End index.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.end);  // -> 50
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.end);  // -> 50
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.end);  // -> 50
   * ```
   * @type {Integer} Initial ending point of the segment.
   */

  this.end = end;
}

/**
 * @namespace WaveformDataSegment
 */

WaveformDataSegment.prototype = {
  /**
   * Dynamic starting point based on the WaveformData instance offset.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.offset_start);  // -> 10
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.offset_start);  // -> 20
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.offset_start);  // -> null
   * ```
   *
   * @return {number} Starting point of the segment within the waveform offset. (inclusive)
   */

  get offset_start() {
    if (this.start < this.context.offset_start && this.end > this.context.offset_start) {
      return this.context.offset_start;
    }

    if (this.start >= this.context.offset_start && this.start < this.context.offset_end) {
      return this.start;
    }

    return null;
  },

  /**
   * Dynamic ending point based on the WaveformData instance offset.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.offset_end);  // -> 50
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.offset_end);  // -> 50
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.offset_end);  // -> null
   * ```
   *
   * @return {number} Ending point of the segment within the waveform offset. (exclusive)
   */

  get offset_end() {
    if (this.end > this.context.offset_start && this.end <= this.context.offset_end) {
      return this.end;
    }

    if (this.end > this.context.offset_end && this.start < this.context.offset_end) {
      return this.context.offset_end;
    }

    return null;
  },

  /**
   * Dynamic segment length based on the WaveformData instance offset.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.offset_length);  // -> 40
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.offset_length);  // -> 30
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.offset_length);  // -> 0
   * ```
   *
   * @return {number} Visible length of the segment within the waveform offset.
   */

  get offset_length() {
    return this.offset_end - this.offset_start;
  },

  /**
   * Initial length of the segment.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.length);  // -> 40
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.length);  // -> 40
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.length);  // -> 40
   * ```
   *
   * @return {number} Initial length of the segment.
   */

  get length() {
    return this.end - this.start;
  },

  /**
   * Indicates if the segment has some visible part in the actual WaveformData offset.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.visible);        // -> true
   *
   * waveform.offset(20, 50);
   * console.log(waveform.segments.example.visible);        // -> true
   *
   * waveform.offset(70, 100);
   * console.log(waveform.segments.example.visible);        // -> false
   * ```
   *
   * @return {Boolean} True if at least partly visible, false otherwise.
   */

  get visible() {
    return this.context.in_offset(this.start) ||
           this.context.in_offset(this.end) ||
           (this.context.offset_start > this.start && this.context.offset_start < this.end);
  },

  /**
   * Return the minimum values for the segment.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.min.length);        // -> 40
   * console.log(waveform.segments.example.min.offset_length); // -> 40
   * console.log(waveform.segments.example.min[0]);            // -> -12
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.segments.example.min.length);        // -> 40
   * console.log(waveform.segments.example.min.offset_length); // -> 30
   * console.log(waveform.segments.example.min[0]);            // -> -5
   * ```
   *
   * @return {Array.<Integer>} Min values of the segment.
   */

  get min() {
    return this.visible ? this.context.offsetValues(this.offset_start, this.offset_length, 0) : [];
  },

  /**
   * Return the maximum values for the segment.
   *
   * ```javascript
   * var waveform = new WaveformData({ ... }, WaveformData.adapters.object);
   * waveform.set_segment(10, 50, "example");
   *
   * console.log(waveform.segments.example.max.length);        // -> 40
   * console.log(waveform.segments.example.max.offset_length); // -> 40
   * console.log(waveform.segments.example.max[0]);            // -> 5
   *
   * waveform.offset(20, 50);
   *
   * console.log(waveform.segments.example.max.length);        // -> 40
   * console.log(waveform.segments.example.max.offset_length); // -> 30
   * console.log(waveform.segments.example.max[0]);            // -> 11
   * ```
   *
   * @return {Array.<Integer>} Max values of the segment.
   */

  get max() {
    return this.visible ? this.context.offsetValues(this.offset_start, this.offset_length, 1) : [];
  }
};

module.exports = WaveformDataSegment;

},{}],8:[function(require,module,exports){
"use strict";

var WaveformData = require("./lib/core");

WaveformData.adapters = require("./lib/adapters");

module.exports = WaveformData;

},{"./lib/adapters":3,"./lib/core":5}],9:[function(require,module,exports){
module.exports={
  "version": "0.0.1"
}
},{}],10:[function(require,module,exports){
var package_json = require('../package.json'),
	Settings = require('./settings'),
	Do = require('do.js');
	utils = require("./utils");
	WaveformData = require("waveform-data");

// Data Store with a source of truth
function DataStore() {
	this.data = undefined;
	this.ui = undefined;

	this.initData = function initData(audio, tracks){
		this.data = audio;

		// for(var i=0; i<this.data; i++){
		// 	this.data[i].original_length = (this.data[i].end)*Settings.time_scale - (this.data[i].start)*Settings.time_scale
		// }

		var lineWidth, lineHeight = utils.getDivSize("timeline");
		lineHeight = lineHeight * Settings.lineHeightProportion;

		this.ui = {
			currentTime: 0,
			totalTime: Settings.default_length,
			scrollTime: 0,
			timeScale: Settings.time_scale,
			tracks: tracks,
			trackTimelineOffset: Settings.trackTimelineOffset, //Offset between top of studio timeline and start of track items
			lineHeight: lineHeight, //Size of track items
			xScrollTime: 0
		};
		this.fetchWaveFormData();
	}

	this.updateData = function updateData(audioId, key, value) {
		for (var i in this.data) {
			if (this.data[i].id == audioId) {
				this.data[i][key] = value;
				break;
			}
		}
	}

	this.addEffect = function addEffect(audioId, effectObject) {
		for (var i in this.data) 
			if (this.data[i].id == audioId){
				this.data[i].effects.push(effectObject)
				break;
			}
	}

	this.updateEffect = function updateEffect(audioId, effectId, effectObject) {
		for (var i in this.data){
			if (this.data[i].id == audioId){
				for (var i2 in this.data[i].effects){
					if (this.data[i].effects[i2].id == effectId){
						this.data[i].effects[i2] = effectObject;
						break;
					}
				}
			}
		}
	}

	this.updateUi = function updateUi(key, value) {
		this.ui[key] = value
	}

	this.getData = function getData(type, key) {
		if (type == "data"){
			return this.data;

		} else if (type == "ui"){
			if (key == undefined) {
				return this.ui;

			} else {
				return this.ui[key];
			}
		}
	}

	this.fetchWaveFormData = function fetchWaveFormData(){
		var dataTmp = this;

		for (var i=0; i<this.data.length; i++){
			if (this.data[i].wave_form != null){
				console.log("Running for", this.data[i].id)
				var xhttp = new XMLHttpRequest();
				xhttp.open('GET', "http://localhost:8000/"+this.data[i].wave_form);
				xhttp.i = i;
				xhttp.responseType = 'arraybuffer';

				xhttp.onload = function(data) {
					dataTmp.data[0].raw_wave_form = WaveformData.create(data.target); //currently not updating dynamically because js fucking sucks
					dataTmp.data[1].raw_wave_form = WaveformData.create(data.target);
				};

				xhttp.send();
			}
		}
	}
}

module.exports = DataStore;

},{"../package.json":9,"./settings":13,"./utils":20,"do.js":1,"waveform-data":8}],11:[function(require,module,exports){
/**************************/
// Dispatcher
/**************************/

function Dispatcher() {

	var event_listeners = {

	};

	function on(type, listener) {
		if (!(type in event_listeners)) {
			event_listeners[type] = [];
		}
		var listeners = event_listeners[type];
		listeners.push(listener);
	}

	function fire(type) {
		var args = Array.prototype.slice.call(arguments);
		args.shift();
		var listeners = event_listeners[type];
		if (!listeners) return;
		for (var i = 0; i < listeners.length; i++) {
			var listener = listeners[i];
			listener.apply(listener, args);
		}
	}

	this.on = on;
	this.fire = fire;

}

module.exports = Dispatcher;
},{}],12:[function(require,module,exports){
var Settings = require("./settings");
	utils = require("./utils.js");

function computeHighLow(start, end, type){
	//Computes high/low ratio (0-100) of where the start/target of the effects are compared to possible min/max
	//get min/max of start/end of given effect type
	//normalize min/max so -> min = 0 , max = max + offset of min to get to 0
	//then ratio will be max / (start/end) + offset of min to get to 0
	offset = 0;
	bounds = Settings.effectBounds[type];
	if (bounds["startMin"] <= 0){
		offset = +bounds["startMin"]
	} else {
		offset = -bounds["startMin"]
	}

	max = bounds["startMax"] + offset
	startOff = start + offset;
	endOffset = end + offset;

	if (startOff == 0){
		return [0, (endOffset / max)*100]

	} else if (endOffset == 0){
		return [(startOff / max)*100, 0]

	} else {
		return [(startOff / max)*100, (endOffset / max)*100]
	}
}

function computeEffectsX(effects, startX, time_scale, frame_start){
	for (var i=0; i<effects.length; i++){
		effects[i]["startX"] = startX + effects[i]["start"] * time_scale;
		effects[i]["endX"] = startX + effects[i]["end"] * time_scale;
	}
	return effects;
}

module.exports = {
	computeHighLow: computeHighLow,
	computeEffectsX: computeEffectsX
}
},{"./settings":13,"./utils.js":20}],13:[function(require,module,exports){
//Time scale definitions
var DEFAULT_TIME_SCALE = 60;

//Timeline component sizes (% of screen size)
var timelineToolbarHeight = 0.05
var timelineToolbarWidth = 1

var trackColumnWidth = 0.2
var trackColumnHeight = 0.85

var timelineWidth = 0.8
var timelineHeight = 0.85

var topTimelineWidth = 0.8
var topTimelineHeight = 0.6

var lineHeightProportion = 0.22 //This value should be worked out on size of studio vs number of tracks so that we can fill the entire space
var trackTimelineOffset = 40;

//min/max bounds for start/end of effects - EQ is missing min/max bounds for start/target-decibels
var effectBounds = {"volume": {"startMin": 0, "startMax": 1, "endMin": 0, "endMax": 1}, 
                    "high_pass_filter": {"startMin": 20, "startMax": 15000, "endMin": 20, "endMax": 15000}, 
                    "low_pass_filter": {"startMin": 15000, "startMax": 20, "endMin": 15000, "endMax": 20}, 
                    "eq": {"startMin": 0, "startMax": 2, "endMin": 0, "endMax": 2}, 
                    "pitch": {"startMin": -12, "startMax": 12, "endMin": -12, "endMax": 12}, 
                    "tempo": {"startMin": 0, "startMax": 250, "endMin": 0, "endMax": 250}}
                    // "gain": {"startMin": , "startMax": , "endMin": , "endMax": }, "flanger": {"startMin": , "startMax": , "endMin": , "endMax": }, 
                    // "echo": {"startMin": , "startMax": , "endMin": , "endMax": }, "phaser": {"startMin": , "startMax": , "endMin": , "endMax": }, 
                    // "reverb": {"startMin": , "startMax": , "endMin": , "endMax": }}

// Dimensions
module.exports = {
	MARKER_TRACK_HEIGHT: 60,
	TIMELINE_SCROLL_HEIGHT: 0,
	time_scale: DEFAULT_TIME_SCALE, // number of pixels to 1 second
    default_length: 600, // seconds
    trackColumnWidth: trackColumnWidth,
    trackColumnHeight: trackColumnHeight,
    timelineWidth: timelineWidth,
    timelineHeight: timelineHeight,
    topTimelineWidth: topTimelineWidth,
    topTimelineHeight: topTimelineHeight,
    timelineToolbarHeight: timelineToolbarHeight,
    timelineToolbarWidth: timelineToolbarWidth,
    lineHeightProportion: lineHeightProportion,
    trackTimelineOffset: trackTimelineOffset,
    effectBounds: effectBounds
};

},{}],14:[function(require,module,exports){
var ui = require("./ui"),
	Dispatcher = require("./dispatcher"),
	dataStore = require("./data-store"),
	timelineScroll = require("./ui-scroll"),
	timeline = require("./ui-main-timeline");
	Settings = require("./settings");

function Studio(audio){
	//Accepts audio and tracks. Audio is a json array of objects of each audio item to be rendered into timeline view. 
	ui.initCanvas(); //Create intial divs with canvas items inside for drawing

	//Create event listeners here to update various variables so that when paint() is called the items are painted dynamically according to what 
	//the user is doing? 
	var tracks = Math.max.apply(Math, audio.map(function(o) { return o.track; }))+1; //Maximum number of tracks which audio occupies +1 to account for starting at 0
	var needsResize = false;
	var data = new dataStore();
	data.initData(audio, tracks);

	var dispatcher = new Dispatcher();
	var timelineObject = new timeline.timeline(data, dispatcher);

	//Setting dispatcher functions
	dispatcher.on('time.update', function(v) {
		v = Math.max(0, v);
		data.updateUi("currentTime", v);

		// if (start_play) start_play = performance.now() - v * 1000;
		// repaintAll();
		// layer_panel.repaint(s);
	});	
	dispatcher.on('update.scrollTime', function(v) {
		v = Math.max(0, v);
		data.updateUi("scrollTime", v);
		// repaintAll();
	});
	dispatcher.on('update.scale', function(v) {
		console.log('range', v);
		data.updateUi("timeScale", v);
	});

	dispatcher.on("update.audioTime", function(id, start, end){
		console.trace();
		data.updateData(id, "start", start);
		data.updateData(id, "end", end);
	});

	dispatcher.on("update.audioTrack", function(id, track){
		data.updateData(id, "track", track);
	})

	//Registering event listeners
	window.addEventListener('resize', function() {
		needsResize = true;
	});

	//Core paint routines
	function paint() {
		requestAnimationFrame(paint);
		if (needsResize == true){
			timelineObject.resize();
			needsResize = false;

		}
		timelineObject.paint();
	}

	paint();
}

window.Studio = Studio;
},{"./data-store":10,"./dispatcher":11,"./settings":13,"./ui":19,"./ui-main-timeline":17,"./ui-scroll":18}],15:[function(require,module,exports){
module.exports = {
	// photoshop colors
	a: '#343434',
	b: '#535353',
	c: '#b8b8b8',
	d: '#d6d6d6',
	audioElement: '#4286f4',
	effectColours: {"volume": "#15a071", "high_pass_filter": "#ffff00", "low_pass_filter": "#ffae19", "eq": "#FF5733", "pitch": "#3933FF", 
					"tempo": "#FF33BB", "gain": "#FF3333", "flanger": "#1DFF2B", "echo": "#1DC8FF", "phaser": "#AD1DFF", 
					"reverb": "#FF1DA6"}
};
},{}],16:[function(require,module,exports){
var Settings = require("./settings");
	utils = require("./utils");

function Rect() {
	
}

Rect.prototype.set = function(x, y, w, h, color, outline) {
	this.x = x;
	this.y = y;
	this.w = w;
	this.h = h;
	this.color = color;
	this.outline = outline;
};

Rect.prototype.paint = function(ctx) {
	ctx.fillStyle = Theme.b;  // // 'yellow';
	ctx.strokeStyle = Theme.c;

	this.shape(ctx);

	ctx.stroke();
	ctx.fill();
};

Rect.prototype.shape = function(ctx) {
	ctx.beginPath();
	ctx.rect(this.x, this.y, this.w, this.h);
};

Rect.prototype.contains = function(x, y) {
	return x >= this.x && y >= this.y
	 && x <= this.x + this.w && y <= this.y + this.h;
};

function zoom(dataStore, dispatcher){
	var div = document.getElementById("zoomColumn");
	
	this.resize = function () {
		var range = document.getElementById("rangeSlider");
		utils.style(range, {
			width: (div.offsetWidth-20).toString() + "px",
		});
	}

	function changeRange() {
		dispatcher.fire('update.scale', range.value);
	}

	var range = document.createElement('input');
	range.setAttribute("id", "rangeSlider");
	range.type = "range";
	range.value = Settings.time_scale;
	range.min = 1;
	range.max = 100;
	range.step = 0.5;

	utils.style(range, {
		width: (div.offsetWidth-20).toString() + "px"
	});

	var draggingRange = 0;

	range.addEventListener('mousedown', function() {
		draggingRange = 1;
	});

	range.addEventListener('mouseup', function() {
		draggingRange = 0;
		changeRange();
	});

	range.addEventListener('mousemove', function() {
		if (!draggingRange) return;
		changeRange();
	});

	div.appendChild(range)

}

function trackCanvas(dataStore, dispatcher){
	var canvas = document.getElementById("left-column-canvas");
	var width = canvas.width;
	var height = canvas.height;
	var dpr = window.devicePixelRatio; 
	var ctx = canvas.getContext('2d');
	var zoomBar = new zoom(dataStore, dispatcher);

	this.resize = function() {
		parentDiv = document.getElementById("left-column")
		canvas.width = parentDiv.offsetWidth;
		canvas.height = parentDiv.offsetHeight;
		height = canvas.height;
		width = canvas.width;
		zoomBar.resize();
	}

	this.paint = function() {
		var trackLayers = dataStore.getData("ui", "tracks");
		var lineHeight = dataStore.getData("ui", "lineHeight"); //TODO line height should be updated as more track layers are added - if track layers extend view
		var offset = dataStore.getData("ui", "trackTimelineOffset");

		ctx.fillStyle = Theme.a;
		ctx.fillRect(0, 0, width, height);
		ctx.save();
		ctx.scale(dpr, dpr);

		for (var i = 0; i <= trackLayers; i++){
			ctx.strokeStyle = Theme.b;
			ctx.beginPath();
			ctx.moveTo(0, ((offset + i*lineHeight)/dpr)+i);
			ctx.lineTo(width, ((offset + i*lineHeight)/dpr)+i);
			ctx.stroke();

			if (i != 0){
				ctx.fillStyle = Theme.d;
				ctx.textAlign = 'center';
				ctx.fillText(i.toString(), width/4, ((offset + i*lineHeight)/dpr)-(lineHeight/4));
			}
		}
		ctx.restore();
	}
}

module.exports = {
	trackCanvas: trackCanvas
};
},{"./settings":13,"./utils":20}],17:[function(require,module,exports){
var Settings = require("./settings");
	utils = require("./utils");
	proxy_ctx = utils.proxy_ctx,
	Theme = require("./theme");
	timelineScroll = require("./ui-scroll");
	uiExterior = require("./ui-exterior");
	effectUtils = require("./effects.js");
//Import settings/functions from other files

var tickMark1;
var tickMark2;
var tickMark3;
var frame_start;

//AudioItem class? which is used to draw each audio item + x/y containing operations 
function AudioItem() {
	
}

const interpolateHeight = (total_height, offset) => {
  const amplitude = 256;
  return (size) => total_height - ((size + 128) * total_height) / amplitude;
};

//Set variables for audio item
//This should be refactored to accept AudioItem object and then this variables set from getting values from this object - much cleaner than sending a bunch of paramters
AudioItem.prototype.setWaveForm = function(rawWaveForm, y, y2, x, x2, time_scale, frame_start, offset, dpr){
	this.rawWaveForm = rawWaveForm;
	this.rawWaveFormMin = [];
	this.rawWaveFormMax = [];
	this.y = y;
	this.y2 = y2;
	this.x = x;
	this.x2 = x2;
	this.xNormalized = this.x + (frame_start * time_scale);
	this.x2Normalized = this.x2 + (frame_start * time_scale);
	this.size = this.x2Normalized - this.xNormalized;

	if (this.rawWaveForm != null){
		const y = interpolateHeight(this.y2-16);
		this.rawWaveForm = this.rawWaveForm.resample({ width: this.size })

		// for(var i=0; i<this.rawWaveForm.min.length; i++){
		// 	this.rawWaveFormMin.push([i + 0.5, y(this.rawWaveForm.min[i]) + 0.5])
		// }
		this.rawWaveForm.min.forEach((val, x) => {
		  this.rawWaveFormMin.push([x + 0.5, y(val)+8])
		});
		// this.rawWaveForm.max = this.rawWaveForm.max.reverse()
		// for(var i=0; i<this.rawWaveForm.max.length; i++){
		// 	this.rawWaveFormMax.push([(this.rawWaveForm.offset_length - y) + 0.5, y(this.rawWaveForm.max[i]) + 0.5]);
		// }
		this.rawWaveForm.max.reverse().forEach((val, x) => {
			this.rawWaveFormMax.push([(this.rawWaveForm.offset_length - x) + 0.5, y(val)+8]);
		});

	}
}

AudioItem.prototype.set = function(x, y, x2, y2, color, audioName, id, track, time_scale, frame_start, barMarkers, effects) {
	this.x = x;
	this.y = y;
	this.x2 = x2;
	this.y2 = y2;
	this.color = color;
	this.audioName = audioName;
	this.id = id;
	this.track = track;
	this.xNormalized = x + (frame_start * time_scale);
	this.x2Normalized = x2 + (frame_start * time_scale);
	this.size = this.x2Normalized - this.xNormalized;
	this.ySize = this.y2 - this.y;
	this.xMiddle = this.xNormalized + ((this.size) / 2);
	this.effects = effects;
	this.barMarkers = barMarkers;
	this.time_scale = time_scale;
	this.frame_start = frame_start;
	this.xOffset = this.frame_start * this.time_scale;
	this.ratio = this.y2 / 100;

	this.rounded1X = utils.round(this.xNormalized, 0.25);
	this.rounded1X2 = utils.round(this.x2Normalized, 0.25);

	this.rounded2X = utils.round(this.xNormalized, 0.5);
	this.rounded2X2 = utils.round(this.x2Normalized, 0.5);

	this.rounded3X = utils.round(this.xNormalized, 1);
	this.rounded3X2 = utils.round(this.x2Normalized, 1);
};

AudioItem.prototype.updateBars = function(startX, draggingx){
	this.barMarkersX, this.barMarkersXRounded = utils.increaseArray(this.barMarkersX, (startX-draggingx), true);
}

AudioItem.prototype.createBarDiff = function(){
	this.barMarkerDiff = {};
	for (var i=0; i<this.barMarkersX.length; i++){
		this.barMarkerDiff[this.barMarkersX[i]] = [this.barMarkersX[i]-this.xNormalized, this.x2Normalized-this.barMarkersX[i]];
	}
}

AudioItem.prototype.paintWaveform = function(ctx){
	if (this.rawWaveForm != undefined){
		// //console.log(this.rawWaveForm)
		//lets add some checking to this rendering which will check if X value is greater/less than canvas width -> if so then dont render only render what is in view
		//pre rendering?
		ctx.beginPath();
		// from 0 to 100
		//compute waveform min/max upon the setWaveForm function - so we dont need to recompute for each iteration over the min/max values
		for (var i=0; i<this.rawWaveFormMin.length; i++){
			ctx.lineTo(this.xNormalized+this.rawWaveFormMin[i][0] - this.xOffset, this.y+this.rawWaveFormMin[i][1])
		}
		// this.rawWaveForm.min.forEach((val, x) => {
		//   ctx.lineTo(x + 0.5, y(val) + 0.5);
		// });

		for (var i=0; i<this.rawWaveFormMax.length; i++){
			ctx.lineTo(this.xNormalized+this.rawWaveFormMax[i][0] - this.xOffset, this.y+this.rawWaveFormMax[i][1])
		}
		// then looping back from 100 to 0
		// this.rawWaveForm.max.reverse().forEach((val, x) => {
		//   ctx.lineTo((this.rawWaveForm.offset_length - x) + 0.5, y(val) + 0.5);
		// });

		ctx.closePath();
		ctx.stroke();
		ctx.fill();
	}
}

AudioItem.prototype.paintEffects = function(ctx) {
	//Iterate over effects and paint them on the audio item - in theme define some basic temporary colour scheme/symbols which can signify different effects
	//So for example yellow = eq, red = volume. etc. Currently only two supported strength_curves on backend: continous and linear - just build these for now
	//Y position on the audio item should indicate the start/target values of the effect
	for (var i=0; i<this.effects.length; i++){
		effect = this.effects[i];
		//currently start/target only works for phase 1 of effect - this should be able to handle effects which have multiple start/targets - 
		//maybe this means drawing multiple curves?
		var effectStartRatio, effectEndRatio;
		out = effectUtils.computeHighLow(effect["params"]["start"], effect["params"]["target"], effect["type"]);
		effectStartRatio = out[0];
		effectEndRatio = out[1];
		effectStartY = this.y + this.y2 - effectStartRatio * this.ratio;
		effectEndY = this.y + this.y2 - effectEndRatio * this.ratio;
		console.log(effectStartRatio, effectEndRatio);
		console.log(effectStartY, effectEndY);
		console.log(effect["startX"], effect["endX"]);
		//now we need to start/end ratios and figure out the Y value which would be associated 
		//then draw Y lines at effect startX/endX with either continous or linear line connecting the two
		//where start/end of the line are computed from the ratios generated above
		ctx.strokeStyle = Theme.effectColours[effect["type"]];
		ctx.beginPath();
		ctx.moveTo(effect["startX"], this.y);
		ctx.lineTo(effect["startX"], this.y+this.y2);
		ctx.stroke();
		ctx.moveTo(effect["endX"], this.y);
		ctx.lineTo(effect["endX"], this.y+this.y2);
		ctx.stroke();

		if (effect["params"]["strength_curve"] == "linear"){
			ctx.moveTo(effect["startX"], effectStartY);
			ctx.lineTo(effect["endX"], effectEndY)
			ctx.stroke();

		} else if (effect["params"]["strength_curve"] == "continous"){
			ctx.moveTo(effect["startX"], effectEndY);
			ctx.lineTo(effect["endX"], effectEndY)
			ctx.stroke();
		}
	}
}

AudioItem.prototype.paintBarMarkers = function(ctx) {
	this.barMarkersX = [];
	this.barMarkersXRounded = [];
	ctx.strokeStyle = "grey";

	for (var i=0; i<this.barMarkers.length; i++){
		if (i % 4 == 0){ ctx.lineWidth = 2; } else { ctx.lineWidth = 1;}

		time = utils.time_to_x(this.barMarkers[i], this.time_scale, this.frame_start) + this.xNormalized;
		this.barMarkersX.push(time);
		this.barMarkersXRounded.push(utils.round(time, 0.5));
		ctx.beginPath();
		ctx.moveTo(time, this.y+1);
		ctx.lineTo(time, this.y+this.y2);
		ctx.fillText(i+1, time+5, this.y+this.y2-1);
		ctx.stroke();
	}
	this.createBarDiff();
	ctx.lineWidth = 1.0;
}

//Paint audio item in canvas
AudioItem.prototype.paint = function(ctx, outlineColor) {
	ctx.fillStyle = outlineColor;
	ctx.beginPath();
	ctx.rect(this.x, this.y, this.x2-this.x, this.y2);
	ctx.fill();
	ctx.strokeStyle = "black";
	ctx.stroke();
	ctx.fillStyle = "black";
	txtWidth = ctx.measureText(this.audioName).width;
	if (txtWidth < this.x2-this.x){ctx.fillText(this.audioName, this.x+txtWidth, this.y+10);}
	this.paintWaveform(ctx);
	this.paintBarMarkers(ctx);
	this.paintEffects(ctx);
};

//Check if mouse at x/y is contained in audio
AudioItem.prototype.contains = function(x, y, time_scale, frame_start) {
	// console.log("X", this.x, "Y", this.y, "X2", this.x2, "y2", this.y2)
	// console.log("Comparison", x ," >= ", (this.x + (frame_start * time_scale)), y, " >= ", this.y, x, " <= ", (this.x2 + (frame_start * time_scale)), y, "<= ", this.y + this.y2, this.id)
	//X & X2 values of audio item are normalized in accordance with the timescale and framestart so we can effectively care againsy mouse position no matter where the scroll wheel is
	return x >= this.xNormalized && y >= this.y  && x <= this.x2Normalized && y <= this.y + this.y2;
};

//Change outline to red to notify user that they cannot slide audio over item in same track
AudioItem.prototype.alert = function(ctx, outlineColor){
	this.paint(ctx, outlineColor);
}


//Gets the timescale values for each tick
function time_scaled(time_scale) {
	/*
	 * Subdivison LOD
	 * time_scale refers to number of pixels per unit
	 * Eg. 1 inch - 60s, 1 inch - 60fps, 1 inch - 6 mins
	 */
	var div = 60;

	tickMark1 = time_scale / div;
	tickMark2 = 2 * tickMark1;
	tickMark3 = 10 * tickMark1;

}

function timeline(dataStore, dispatcher) {
	var canvas = document.getElementById("timeline-canvas");
	var ctx = canvas.getContext('2d');
	var dpr = window.devicePixelRatio; 
	var scroll_canvas = new timelineScroll.timelineScroll(dataStore, dispatcher); //Creates timeline scroll and gets object of timeline scroll
	var track_canvas = new uiExterior.trackCanvas(dataStore, dispatcher); //Creates exterior track canvas and gets object
	var time_scale;
	var renderItems = [];
	var renderedItems = false;
	var drawSnapMarker = 0;
	var trackLayers = dataStore.getData("ui", "tracks");
	var lineHeight = dataStore.getData("ui", "lineHeight");
	var offset = dataStore.getData("ui", "trackTimelineOffset");
	var trackBounds = {};
	var height = canvas.height;
	var width = canvas.width;
	var audioData = dataStore.getData("data", "data");
	var time_scale = dataStore.getData("ui", "timeScale");
	var lastTimeScale = time_scale;
	var resetWaveForm = false;

	//console.log("Before move data", dataStore.getData("data"));

	//Create array of objects which defines the pixel bounds for each track element
	for (var i=0; i<trackLayers; i++){
		trackBounds[i] = [(offset + i*lineHeight)/dpr, (offset + (i+1)*lineHeight)/dpr];
	}

	//Resize function called upon window resize - will resize canvas so that future paint operations can be correctly painted according to resize
	function resize() {
		parentDiv = document.getElementById("timeline")
		height = parentDiv.offsetHeight;
		width = parentDiv.offsetWidth;
		canvas.height = height;
		canvas.width = width;
		dataStore.updateUi("lineHeight", height*Settings.lineHeightProportion); //Update lineHeight in accordance to window height + proportion of track to window
		scroll_canvas.resize();
		track_canvas.resize();
		resetWaveForm = true;

		//Redefine track bounds after resize
		for (var i=0; i<trackLayers; i++){
			trackBounds[i] = [(offset + i*lineHeight)/dpr, (offset + i+1*lineHeight)/dpr];
		}
	}

	//Core paint routine for studio view
	function paint() {
		//Paint other canvas items
		scroll_canvas.paint();
		track_canvas.paint();

		var time_scale = dataStore.getData("ui", "timeScale");
		time_scaled(time_scale);
		currentTime = dataStore.getData("ui", "currentTime"); // of marker
		frame_start = dataStore.getData("ui", "scrollTime"); //Starting time value of timeline view

		var units = time_scale / tickMark1; //For now timescale is taken from settings - this should be updated later as user zooms into timeline
		var offsetUnits = (frame_start * time_scale) % units;
		var count = (canvas.width + offsetUnits) / tickMark1; //Amount of possible main tick markers across window width

		//TODO: Lines and text size should scale relative to size of canvas
		ctx.fillStyle = Theme.a;
		ctx.fillRect(0, 0, width, height);
		ctx.save();
		ctx.scale(dpr, dpr);

		ctx.lineWidth = 1;

		//Iterate over count and draw main tick markers along with second(s) timestamp related to this
		for (i = 0; i < count; i++) {
			x = (i * units) - offsetUnits;

			// vertical lines
			ctx.strokeStyle = Theme.b;
			ctx.beginPath();
			ctx.moveTo(x, 5);
			ctx.lineTo(x, height);
			ctx.stroke();

			ctx.fillStyle = Theme.d;
			ctx.textAlign = 'center';

			//Get time at current tick with accordance to timescale and scroll wheel position
			var t = (i * units - offsetUnits) / time_scale + frame_start;
			if (t != 0){
				t = utils.format_friendly_seconds(t);
				ctx.fillText(t, x, 15);
			}
		}
		units = time_scale / tickMark2;
		count = (width + offsetUnits) / units;

		// marker lines - main
		for (i = 0; i < count; i++) {
			ctx.strokeStyle = Theme.d;
			ctx.beginPath();
			x = i * units - offsetUnits;
			ctx.moveTo(x, 0);
			ctx.lineTo(x, 8);
			ctx.stroke();
		}

		var mul = tickMark3 / tickMark2;
		units = time_scale / tickMark3;
		count = (width + offsetUnits) / units;

		// small ticks
		for (i = 0; i < count; i++) {
			if (i % mul === 0) continue;
			ctx.strokeStyle = Theme.c;
			ctx.beginPath();
			x = i * units - offsetUnits;
			ctx.moveTo(x, 0);
			ctx.lineTo(x, 5);
			ctx.stroke();
		}

		drawAudioElements(); //Draw audio elements
		ctx.restore();

		//Begin drawing marker
		var lineHeight = dataStore.getData("ui", "lineHeight");
		ctx.strokeStyle = 'red'; // Theme.c
		x = ((currentTime - (frame_start)) * time_scale)*dpr;

		//Get currentTime to input into marker
		var txt = utils.format_friendly_seconds(currentTime);
		var textWidth = ctx.measureText(txt).width;

		var base_line = (lineHeight, half_rect = textWidth / dpr);

		//Draw main line 
		ctx.beginPath();
		ctx.moveTo(x, base_line);
		ctx.lineTo(x, height);
		ctx.stroke();

		//Draw main marker body
		ctx.fillStyle = 'red'; // black
		ctx.textAlign = 'center';
		ctx.beginPath();
		ctx.moveTo(x, base_line + 5);
		ctx.lineTo(x + 5, base_line);
		ctx.lineTo(x + half_rect, base_line);
		ctx.lineTo(x + half_rect, base_line - 14);
		ctx.lineTo(x - half_rect, base_line - 14);
		ctx.lineTo(x - half_rect, base_line);
		ctx.lineTo(x - 5, base_line);
		ctx.closePath();
		ctx.fill();

		ctx.fillStyle = 'white';
		ctx.fillText(txt, x, base_line - 4);

		ctx.restore();

		needsRepaint = false;

	}

	function drawAudioElements() {
		var trackLayers = dataStore.getData("ui", "tracks");
		var lineHeight = dataStore.getData("ui", "lineHeight"); //TODO line height should be updated as more track layers are added - if track layers extend view
		var offset = dataStore.getData("ui", "trackTimelineOffset");
		var audioData = dataStore.getData("data", "data");
		var time_scale = dataStore.getData("ui", "timeScale");
		var y;

		//Draw track lines
		for (var i = 0; i <= trackLayers; i++){
			y = (offset + i*lineHeight)/dpr;
			ctx.strokeStyle = Theme.b;
			ctx.beginPath();
			ctx.moveTo(0, y);
			ctx.lineTo(width, y);
			ctx.stroke();
		}

		//Iterate over audioData and paint componenets on timeline - along will all effects associated on them
		for (var i = 0; i < audioData.length; i++){
			audioItem = audioData[i];
			x = utils.time_to_x(audioItem.start, time_scale, frame_start); //Starting x value for audio item
			x2 = utils.time_to_x(audioItem.end, time_scale, frame_start); //Ending x value for audio item
			audioItem.effects = effectUtils.computeEffectsX(audioItem.effects, x, time_scale, frame_start);
			var y1 = (offset + audioItem.track * lineHeight)/dpr; //Starting y value for audio item
			var y2 = (lineHeight)/dpr; //Ending y value for audio item

			if (renderedItems == false){
				AudioRect = new AudioItem();
				AudioRect.setWaveForm(audioItem.raw_wave_form, y1, y2, x, x2, frame_start, time_scale, offset, dpr);
				AudioRect.set(x, y1, x2, y2, Theme.audioElement, audioItem.name, audioItem.id, audioItem.track, time_scale, frame_start, audioItem.beat_markers, audioItem.effects);
				AudioRect.paint(ctx, Theme.audioElement);
				renderItems.push(AudioRect);

			} else {
				currentItem = renderItems[i];
				if (audioItem.raw_wave_form != null && currentItem.rawWaveForm == undefined || lastTimeScale != time_scale || resetWaveForm == true){
					currentItem.setWaveForm(audioItem.raw_wave_form, y1, y2, x, x2, frame_start, time_scale, offset, dpr);
				}
				currentItem.set(x, y1, x2, y2, Theme.audioElement, audioItem.name, audioItem.id, audioItem.track, time_scale, frame_start, audioItem.beat_markers, audioItem.effects);
				currentItem.paint(ctx, Theme.audioElement);
			}
		}
		lastTimeScale = time_scale;
		renderedItems = true;
		resetWaveForm = false;

		if (drawSnapMarker != false){
			ctx.strokeStyle = "red";
			ctx.beginPath();
			ctx.moveTo(drawSnapMarker - frame_start * time_scale, 0);
			ctx.lineTo(drawSnapMarker - frame_start * time_scale, height);
			ctx.stroke();
		}
	}

	//Handle Y axis track movement
	function move_y(audioItems, currentDragging, yPosition){
		var track = currentDragging.track;
		renderItemsTracks = {};

		for (var i=0; i<audioItems.length; i++){
			currentItem = audioItems[i];

			if (renderItemsTracks.hasOwnProperty(currentItem.track)){
				renderItemsTracks[currentItem.track].push(currentItem);

			} else {
				renderItemsTracks[currentItem.track] = [currentItem];
			}
		}

		for (var i=0; i<trackLayers; i++){
			//Cursor has moved to a new track - update Y track
			if (yPosition > trackBounds[i][0] && yPosition < trackBounds[i][1] && currentDragging.track != i){
				//Write code to ensure track cant change if position will be inside another audio on Y track
				track = i;
			}
		}
		return track;
	}

	function move_x(renderItems, currentDragging, e, draggingx, lastX){
		startX = (draggingx + e.dx/dpr); //tickOffset must be calculated based on diffence between current x value and last x value
		currentDragging.updateBars(startX, draggingx);
		endX = (startX + currentDragging.size)
		rendX = utils.round(endX, 0.5);
		rstartX = utils.round(startX, 0.5);

		audioItemLoop:
		for (var i = 0; i < renderItems.length; i++){
			item = renderItems[i];
			if (item.track == currentDragging.track && item.id != currentDragging.id){ //If to check if comparison items are on same track 
				if (item.xNormalized >= currentDragging.xNormalized){ //If start of current comparison audio is before dragging audio start
					if (endX >= item.xNormalized){ //Check that computed end is greater than comparison audio start 
						if (e.offsetx/dpr <= item.x2){
							endX = item.xNormalized;
							startX = endX - currentDragging.size;
						} else {
							startX = item.x2Normalized;
							endX = startX + currentDragging.size;
						}
					}
				} else if (item.x2Normalized <= currentDragging.xNormalized){
					if (startX <= item.x2Normalized){
						if (e.offsetx/dpr >= item.x){
							startX = item.x2Normalized;
							endX = startX + currentDragging.size;

						} else {
							endX = item.xNormalized;
							startX = endX - currentDragging.size;
						}
					}
				}
			} else if (item.id != currentDragging.id && lastX != 0) { //Run Y aligment - will hold currently dragged audio item at snapped location for x movement ticks
				//Rounding values should change based on time_scale value - when we are far zoomed out 0.5 is too small each scroll steps much larger than 0.5
				//Items should not be able to snap inside other pieces of audio by being close 
				// console.log("Comparing", rstartX, rendX, "With", item.rounded2X, 
				// 	item.rounded2X2, "ID", item.id, "and", currentDragging.id,
				// 	"original values", startX, endX, item.xNormalized, item.x2Normalized)
				if (rendX == item.rounded2X){ //end2start
					block = true;
					blockNumber = 10;
					startX = item.xNormalized - currentDragging.size;
					endX = item.xNormalized;
					drawSnapMarker = item.xNormalized;
					break audioItemLoop;

				} else if (rendX == item.rounded2X2) { //end2end
					block = true;
					blockNumber = 10;
					startX = item.x2Normalized - currentDragging.size;
					endX = item.x2Normalized;
					drawSnapMarker = item.x2Normalized;
					break audioItemLoop;

				} else if (rstartX == item.rounded2X) { //start2/start
					block = true;
					blockNumber = 10;
					startX = item.xNormalized;
					endX = item.xNormalized + currentDragging.size;
					drawSnapMarker = item.xNormalized;
					break audioItemLoop;

				} else if (rstartX == item.rounded2X2) { //start2end
					block = true;
					blockNumber = 10;
					startX = item.x2Normalized;
					endX = item.x2Normalized + currentDragging.size;
					drawSnapMarker = item.x2Normalized;
					break audioItemLoop;

				} else {
					if (startX >= item.xNormalized && startX <= item.x2Normalized || endX >= item.xNormalized && endX <= item.x2Normalized || item.xNormalized >= startX && item.x2Normalized <= endX){
						if (item.barMarkersX.length > 0){
							for (var i2=0; i2<item.barMarkersXRounded.length; i2++){
								if (rendX == item.barMarkersXRounded[i2]){ //end2start
									block = true;
									blockNumber = 4;
									startX = item.barMarkersX[i2] - currentDragging.size;
									endX = item.barMarkersX[i2];
									drawSnapMarker = item.barMarkersX[i2];
									break audioItemLoop;

								} else if (rstartX == item.barMarkersXRounded[i2]) { //start2/start
									block = true;
									blockNumber = 4;
									startX = item.barMarkersX[i2];
									endX = item.barMarkersX[i2] + currentDragging.size;
									drawSnapMarker = item.barMarkersX[i2];
									break audioItemLoop;

								} else {
									for (var bi=0; bi<currentDragging.barMarkersXRounded.length; bi++){
										if (currentDragging.barMarkersXRounded[bi] == item.barMarkersXRounded[i2]){
											startX = item.barMarkersX[i2] - currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][0];
											endX = item.barMarkersX[i2] + currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][1];
											block = true;
											blockNumber = 4;
											drawSnapMarker = item.barMarkersX[i2];
											break audioItemLoop;

										} else if (currentDragging.barMarkersXRounded[bi] == item.rounded2X) {
											startX = item.xNormalized - currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][0];
											endX = item.xNormalized + currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][1];
											block = true;
											blockNumber = 4;
											drawSnapMarker = item.xNormalized;
											break audioItemLoop;

										} else if (currentDragging.barMarkersXRounded[bi] == item.rounded2X2) {
											startX = item.x2Normalized - currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][0];
											endX = item.x2Normalized + currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][1];
											block = true;
											blockNumber = 4;
											drawSnapMarker = item.x2Normalized;
											break audioItemLoop;
										}
									}
								}
							}
						} else {
							for (var bi=0; bi<currentDragging.barMarkersXRounded.length; bi++){
								if (currentDragging.barMarkersXRounded[bi] == item.rounded2X) {
									startX = item.xNormalized - currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][0];
									endX = item.xNormalized + currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][1];
									block = true;
									blockNumber = 4;
									drawSnapMarker = item.xNormalized;
									break audioItemLoop;

								} else if (currentDragging.barMarkersXRounded[bi] == item.rounded2X2) {
									startX = item.x2Normalized - currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][0];
									endX = item.x2Normalized + currentDragging.barMarkerDiff[currentDragging.barMarkersX[bi]][1];
									block = true;
									blockNumber = 4;
									drawSnapMarker = item.x2Normalized;
									break audioItemLoop;
								}
							}
						}
					}
				}
			}
		}

		//Mouse should not be able to go past 0 - thus if mouse go pasts 0 starts will be updated to 0 and end values will be increased by negative start value to ensure we dont loose length of audio
		if (startX < 0){
			end = end - start;
			endX = endX - startX;
			startX = 0;
			start = 0;
		}
		return startX, endX, block, drawSnapMarker, blockNumber;
	}


	this.paint = paint;
	this.resize = resize;

	//mousemove eventListener to handle cursor changing to pointer upon hovering over a draggable item
	canvas.addEventListener("mousemove", function(e){
		bounds = canvas.getBoundingClientRect();
		var time_scale = dataStore.getData("ui", "timeScale");
		frame_start = dataStore.getData("ui", "scrollTime");

		for (var i = 0; i < renderItems.length; i++){
			item = renderItems[i];
			if (item.contains(((e.clientX - bounds.left)/dpr + (frame_start * time_scale)), (e.clientY - bounds.top)/dpr, time_scale, frame_start)) {
				canvas.style.cursor = 'pointer';
				return;
			}
		}
		canvas.style.cursor = 'default';
	});

	//Right click even listner - will be used for adding effects onto clicked audio item
	canvas.addEventListener('contextmenu', function(e) {
	    e.preventDefault();
	    return false;
	}, false);

	//Handles "wheel" zoom events - trackpad zoom or scroll wheel zoom - also includes scroll left and right
	//Handle scroll left and right - moves timeline left and right - scroll up and down zooms into/out of timeline - then two finger 
	canvas.addEventListener("wheel", function(e){
		xMove = e.deltaX/4;
		var frame_start = dataStore.getData("ui", "scrollTime")
		if (frame_start != 0){
			e.preventDefault();
		}
		dispatcher.fire('update.scrollTime', frame_start + xMove);
	});

	canvas.addEventListener("keydown", function(e){
		console.log("Event called");
		if (e.keyCode == 37){ //left arrow key
			dispatcher.fire('update.scrollTime', frame_start -5);

		} else if (e.keyCode == 39){ //right arrow key
			dispatcher.fire('update.scrollTime', frame_start +5);

		} else if (e.keyCode == 38){ //up arrow key
			var time_scale = dataStore.getData("ui", "timeScale");
			dispatcher.fire('update.scale', time_scale +5);

		} else if (e.keyCode == 40){ //down arrow key
			var time_scale = dataStore.getData("ui", "timeScale");
			dispatcher.fire('update.scale', time_scale +5);

		}
	});

	var draggingx = null;
	var currentDragging = null;
	var holdTick = 0; //Handles snapping of items on y axis to assist with moving tracks together
	var block = false;
	var lastX = 0;
	var startX;
	var endX;
	var trackSave = null; //Saves state of audio items on timeline - to be used if audioItem Y position is put back to previous state - should save
	var trackSave2 = null;
	var blockNumber = 0;

	//Handles dragging of movable items
	utils.handleDrag(canvas,
		function down(e){
			var time_scale = dataStore.getData("ui", "timeScale");
			frame_start = dataStore.getData("ui", "scrollTime");

			for (var i = 0; i < renderItems.length; i++){
				item = renderItems[i];
				if (item.contains(((e.offsetx)/dpr + (frame_start * time_scale)), (e.offsety)/dpr, time_scale, frame_start)) {
					draggingx = item.x + frame_start * time_scale
					currentDragging = item;
					canvas.style.cursor = 'grabbing';
					return;
				}
			}
			dispatcher.fire('time.update', utils.x_to_time((e.offsetx)/dpr, time_scale, frame_start));
		},
		function move(e){
			var time_scale = dataStore.getData("ui", "timeScale");
			frame_start = dataStore.getData("ui", "scrollTime");

			if (draggingx != null) {
				if (block == false){
					canvas.style.cursor = 'grabbing';
					track = move_y(renderItems, currentDragging, e.offsety/dpr);
					currentDragging.track = track;
					startX, endX, block, drawSnapMarker, blockNumber = move_x(renderItems, currentDragging, e, draggingx, lastX);

					//Update x/x2 value of current dragging item so we can use for future compuations
					currentDragging.x = startX;
					currentDragging.x2 = endX;
					currentDragging.xNormalized = startX;
					currentDragging.x2Normalized = endX;
					currentDragging.updateBars(startX, draggingx);
					lastX = startX;
					start = (startX / time_scale);
					end = (endX / time_scale);
					dispatcher.fire('update.audioTime', currentDragging.id, start, end);
					dispatcher.fire('update.audioTrack', currentDragging.id, track);
					//console.log("Post move data", dataStore.getData("data"));

				} else {
					if (holdTick == blockNumber){
						block = false;
						holdTick = 0;
						drawSnapMarker = false;

					} else {
						holdTick += 1;
					}
				}

			} else {
				dispatcher.fire('time.update', utils.x_to_time((e.offsetx)/dpr, time_scale, frame_start));
			}
		},
		function up(e){
			//Reset drag related variables
			draggingx = null;
			currentDragging = null;
			canvas.style.cursor = 'pointer';
			holdTick = 0;
			block = false;
			drawSnapMarker = false;
			blockNumber = 0;
		});
}

module.exports = {
	timeline: timeline
};
},{"./effects.js":12,"./settings":13,"./theme":15,"./ui-exterior":16,"./ui-scroll":18,"./utils":20}],18:[function(require,module,exports){
var Theme = require("./theme")
	utils = require("./utils")

function Rect() {
	
}

Rect.prototype.set = function(x, y, w, h, color, outline) {
	this.x = x;
	this.y = y;
	this.w = w;
	this.h = h;
	this.color = color;
	this.outline = outline;
};

Rect.prototype.paint = function(ctx) {
	ctx.fillStyle = Theme.b;  // // 'yellow';
	ctx.strokeStyle = Theme.c;

	this.shape(ctx);

	ctx.stroke();
	ctx.fill();
};

Rect.prototype.shape = function(ctx) {
	ctx.beginPath();
	ctx.rect(this.x, this.y, this.w, this.h);
};

Rect.prototype.contains = function(x, y) {
	return x >= this.x && y >= this.y
	 && x <= this.x + this.w && y <= this.y + this.h;
};

function timelineScroll(dataStore, dispatcher){
	var canvas = document.getElementById("top-timeline-canvas");
	var ctx = canvas.getContext('2d');
	var dpr = window.devicePixelRatio; 
	var scrollTop = 0, scrollLeft = 0, SCROLL_HEIGHT;
	var layers = dataStore.getData("ui", "layers");

	var TOP_SCROLL_TRACK = 20;
	var MARGINS = 0;

	var scroller = {
		left: 0,
		grip_length: 0,
		k: 1
	};

	var scrollRect = new Rect();
	var height = canvas.height;
	var width = canvas.width;

	this.repaint = function(){
		paint(ctx);
	}

	this.scrollTo = function(s, y) {
		console.log('Scroll to function called arguments are below ')
		console.log(s)
		console.log(y)
		scrollTop = s * Math.max(layers.length * LINE_HEIGHT - SCROLL_HEIGHT, 0);
		repaint();
	};

	this.resize = function resize() {
		parentDiv = document.getElementById("top-timeline")
		canvas.width = parentDiv.offsetWidth;
		canvas.height = parentDiv.offsetHeight;
		height = canvas.height;
		width = canvas.width;
	}

	this.paint = function() {
		var totalTime = dataStore.getData("ui", "totalTime")
		var scrollTime = dataStore.getData("ui", "scrollTime")
		var currentTime = dataStore.getData("ui", "currentTime")
		
		var pixels_per_second = dataStore.getData("ui", "timeScale")

		ctx.save();

		var w = width;
		var h = 16; // TOP_SCROLL_TRACK;
		var h2 = h;

		ctx.fillStyle = Theme.a;
		ctx.fillRect(0, 0, width, height);
		ctx.translate(MARGINS, 5);

		// outline scroller
		ctx.beginPath();
		ctx.strokeStyle = Theme.b;
		ctx.rect(0, height/4, w, height/3);
		ctx.stroke();
		
		var totalTimePixels = totalTime * pixels_per_second;
		var k = w / totalTimePixels;
		scroller.k = k;

		var grip_length = (w * k)/dpr;

		scroller.grip_length = grip_length;

		scroller.left = scrollTime / totalTime * w;
		
		scrollRect.set(scroller.left, height/4, scroller.grip_length, height/3);
		scrollRect.paint(ctx);

		var r = currentTime / totalTime * w;		

		ctx.fillStyle = Theme.c;
		ctx.lineWidth = 2;
		
		ctx.beginPath();
		
		// circle
		// ctx.arc(r, h2 / 2, h2 / 1.5, 0, Math.PI * 2);

		// line
		ctx.rect(r, height/4, 2, height/3);
		ctx.fill()

		ctx.restore();

	}

	/** Handles dragging for scroll bar **/

	var draggingx = null;

	utils.handleDrag(canvas,
		function down(e) {
			console.log("DOWN")
			if (scrollRect.contains(e.offsetx, e.offsety)) {
				draggingx = scroller.left;
				console.log("Dragging x")
				console.log(draggingx);
				return;
			}
			
			var totalTime = dataStore.getData("ui", "totalTime")
			var pixels_per_second = dataStore.getData("ui", "timeScale")
			var frame_start = dataStore.getData("ui", "scrollTime")
			var w = width;

			var t = (e.offsetx) / w * totalTime;

			// data.get('ui:currentTime').value = t;
			dispatcher.fire('time.update', t);
				
		},
		function move (e) {
			if (draggingx != null) {
				console.log("Move");
				var totalTime = dataStore.getData("ui", "totalTime")
				var w = width;

				if ((e.offsetx) < w){ //Check currently does not work - this should check if scroller.start + scroller.length < total width of slider
					dispatcher.fire('update.scrollTime', (draggingx + e.dx)  / w * totalTime);
				}

			} else {
				console.log("DOWN")
				if (scrollRect.contains(e.offsetx, e.offsety)) {
					draggingx = scroller.left;
					console.log("Dragging x")
					console.log(draggingx);
					return;
				}
				
				var totalTime = dataStore.getData("ui", "totalTime")
				var pixels_per_second = dataStore.getData("ui", "timeScale")
				var frame_start = dataStore.getData("ui", "scrollTime")
				var w = width;

				var t = (e.offsetx) / w * totalTime;
				dispatcher.fire('time.update', t);	
			}
		},
		function up(e){
			draggingx = null;
		}
		// function hit(e) {
		// 	if (child.onHit) { child.onHit(e) };
		// }
	);

	canvas.addEventListener("mousedown", function (e){
		bounds = canvas.getBoundingClientRect();
		var currentx = e.clientX - bounds.left, currenty = e.clientY;
	})

	/*** End handling for scrollbar ***/
}

module.exports = {
	timelineScroll: timelineScroll
};
},{"./theme":15,"./utils":20}],19:[function(require,module,exports){
var utils = require("./utils");
	Theme = require("./theme")

function initCanvas () {
	console.log("Running canvas init");
	topToolbar = document.createElement('div');
	topToolbar.setAttribute("id", "top-toolbar");
	utils.style(topToolbar, {position: "fixed", width: "100%", height: "10%"});
	document.body.appendChild(topToolbar);

	topToolbar = document.createElement('div');
	topToolbar.setAttribute("id", "top-timeline");
	utils.style(topToolbar, {position: "fixed", width: "90%", left: "10%", height: "5%", top: "10%"});
	document.body.appendChild(topToolbar);

	canvas = document.createElement('canvas');
	canvas.setAttribute("id", "top-timeline-canvas");
	canvas.style.width ='100%';
	canvas.style.height='100%';
	canvas.width = topToolbar.offsetWidth;
	canvas.height = topToolbar.offsetHeight;
	topToolbar.appendChild(canvas);

	timeline = document.createElement('div');
	timeline.setAttribute("id", "timeline");
	utils.style(timeline, {position: "fixed", left: "10%", top: "15%", width: "90%", height: "85%"});
	document.body.appendChild(timeline);

	canvas = document.createElement('canvas');
	canvas.setAttribute("id", "timeline-canvas");
	canvas.style.width ='101%';
	canvas.style.height='101%';
	canvas.width = timeline.offsetWidth;
	canvas.height = timeline.offsetHeight;
	timeline.appendChild(canvas);

	leftColumn = document.createElement('div');
	leftColumn.setAttribute("id", "left-column");
	utils.style(leftColumn, {position: "fixed", width: "10%", top: "15%", height: "85%"});
	document.body.appendChild(leftColumn);

	canvas = document.createElement('canvas');
	canvas.setAttribute("id", "left-column-canvas");
	canvas.style.width ='100%';
	canvas.style.height='100%';
	canvas.width = leftColumn.offsetWidth;
	canvas.height = leftColumn.offsetHeight;
	leftColumn.appendChild(canvas);

	zoomColumn = document.createElement('div');
	zoomColumn.setAttribute("id", "zoomColumn");
	utils.style(zoomColumn, {position: "fixed", top: "10%", width: "10%", height: "5%", backgroundColor: Theme.a});
	document.body.appendChild(zoomColumn);
}

function paintTrackColumn() {

}

module.exports = {
	initCanvas: initCanvas,
	paintTrackColumn: paintTrackColumn
};
},{"./theme":15,"./utils":20}],20:[function(require,module,exports){
function increaseArray(array, increase, roundFlag){
	out = [];
	outRounded = [];

	for (var i=0; i<array.length; i++){
		if (roundFlag == true){
			v = array[i]+increase;
			out.push(v)
			outRounded.push(round(v));
		}
		out.push(array[i]+increase)
	}
	return out, outRounded;
}

//Convert time in seconds to x value given a timescale
function time_to_x(s, time_scale, frame_start) {
	var ds = s - frame_start;
	ds = ds * time_scale;
	return ds;
}

//Convert x to time given frame start and current time scale
function x_to_time(x, time_scale, frame_start) {
	return frame_start + (x) / time_scale
}

function sortByKey(array, key) {
    return array.sort(function(a, b) {
        var x = a[key]; var y = b[key];
        return ((x < y) ? -1 : ((x > y) ? 1 : 0));
    });
}

function removeFromArrayById(array, id) {
	for (var i=0; i<array.length; i++) {
		if (array[i].id == id) {
			array.splice(i,1);
		}
	}
	return array;
}

function round(value, step) {
	step || (step = 1.0);
	var inv = 1.0 / step;
	return Math.round(value * inv) / inv;
}

function getDivSize(id){
	parentDiv = document.getElementById(id);
	return parentDiv.offsetWidth, parentDiv.offsetHeight;
}

function style(element, var_args) {
	for (var i = 1; i < arguments.length; ++i) {
		var styles = arguments[i];
		for (var s in styles) {
			element.style[s] = styles[s];
		}
	}
}

function format_friendly_seconds(s, type) {
	// TODO Refactor to 60fps???
	// 20 mins * 60 sec = 1080 
	// 1080s * 60fps = 1080 * 60 < Number.MAX_SAFE_INTEGER

	var raw_secs = s | 0;
	var secs_micro = s % 60;
	var secs = raw_secs % 60;
	var raw_mins = raw_secs / 60 | 0;
	var mins = raw_mins % 60;
	var hours = raw_mins / 60 | 0;

	var secs_str = (secs / 100).toFixed(2).substring(2);

	var str = mins + ':' + secs_str;

	if (s % 1 > 0) {
		var t2 = (s % 1) * 60;
		if (type === 'frames') str = secs + '+' + t2.toFixed(0) + 'f';
		else str += ((s % 1).toFixed(2)).substring(1);
		// else str = mins + ':' + secs_micro;
		// else str = secs_micro + 's'; /// .toFixed(2)
	}
	return str;	
}

function proxy_ctx(ctx) {
	// Creates a proxy 2d context wrapper which 
	// allows the fluent / chaining API.
	var wrapper = {};

	function proxy_function(c) {
		return function() {
			// Warning: this doesn't return value of function call
			ctx[c].apply(ctx, arguments);
			return wrapper;
		};
	}

	function proxy_property(c) {
		return function(v) {
			ctx[c] = v;
			return wrapper;
		};
	}

	wrapper.run = function(args) {
		args(wrapper);
		return wrapper;
	};

	for (var c in ctx) {
		// if (!ctx.hasOwnProperty(c)) continue;
		// console.log(c, typeof(ctx[c]), ctx.hasOwnProperty(c));
		// string, number, boolean, function, object

		var type = typeof(ctx[c]);
		switch(type) {
			case 'object':
				break;
			case 'function':
				wrapper[c] = proxy_function(c);
				break;
			default:
				wrapper[c] = proxy_property(c);
				break;
		}
	}

	return wrapper;
}

function handleDrag(element, ondown, onmove, onup, down_criteria) {
	var pointer = null;
	var bounds = element.getBoundingClientRect();
	
	element.addEventListener('mousedown', onMouseDown);

	function onMouseDown(e) {
		handleStart(e);

		if (down_criteria && !down_criteria(pointer)) {
			pointer = null;
			return;
		}

		
		document.addEventListener('mousemove', onMouseMove);
		document.addEventListener('mouseup', onMouseUp);
		
		ondown(pointer);

		e.preventDefault();
	}
	
	function onMouseMove(e) {
		handleMove(e);
		onmove(pointer);
	}

	function handleStart(e) {
		bounds = element.getBoundingClientRect();
		var currentx = e.clientX, currenty = e.clientY;
		pointer = {
			startx: currentx,
			starty: currenty,
			x: currentx,
			y: currenty,
			dx: 0,
			dy: 0,
			offsetx: currentx - bounds.left,
			offsety: currenty - bounds.top,
			moved: false
		};
	}
	
	function handleMove(e) {
		bounds = element.getBoundingClientRect();
		var currentx = e.clientX,
		currenty = e.clientY,
		offsetx = currentx - bounds.left,
		offsety = currenty - bounds.top;
		pointer.x = currentx;
		pointer.y = currenty;
		pointer.dx = e.clientX - pointer.startx;
		pointer.dy = e.clientY - pointer.starty;
		pointer.offsetx = offsetx;
		pointer.offsety = offsety;

		// If the pointer dx/dy is _ever_ non-zero, then it's moved
		pointer.moved = pointer.moved || pointer.dx !== 0 || pointer.dy !== 0;
	}
	
	function onMouseUp(e) {
		handleMove(e);
		onup(pointer);
		pointer = null;
		
		document.removeEventListener('mousemove', onMouseMove);
		document.removeEventListener('mouseup', onMouseUp);
	}

	element.addEventListener('touchstart', onTouchStart);

	function onTouchStart(te) {
		
		if (te.touches.length == 1) {
			
			var e = te.touches[0];
			if (down_criteria && !down_criteria(e)) return;
			te.preventDefault();
			handleStart(e);
			ondown(pointer);
		}
		
		element.addEventListener('touchmove', onTouchMove);
		element.addEventListener('touchend', onTouchEnd);
	}
	
	function onTouchMove(te) {
		var e = te.touches[0];
		onMouseMove(e);
	}

	function onTouchEnd(e) {
		// var e = e.touches[0];
		onMouseUp(e);
		element.removeEventListener('touchmove', onTouchMove);
		element.removeEventListener('touchend', onTouchEnd);
	}


	this.release = function() {
		element.removeEventListener('mousedown', onMouseDown);
		element.removeEventListener('touchstart', onTouchStart);
	};
}

module.exports = {
	style: style,
	format_friendly_seconds: format_friendly_seconds,
	handleDrag: handleDrag,
	getDivSize: getDivSize,
	proxy_ctx: proxy_ctx,
	round: round,
	sortByKey: sortByKey,
	removeFromArrayById: removeFromArrayById,
	time_to_x: time_to_x,
	increaseArray: increaseArray,
	x_to_time: x_to_time
};
},{}]},{},[14]);
